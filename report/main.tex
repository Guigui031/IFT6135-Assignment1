\documentclass{article}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{amsmath}
\usepackage{hyperref}

\title{IFT6135 - H2026}
\author{Guillaume Genois, 20248507}
\date{January 2026}

\begin{document}

\maketitle

\section{Introduction}

%══════════════════════════════════════════════════════════════════════════════
\section{Problem 2}
%══════════════════════════════════════════════════════════════════════════════

\subsection{Question 1: Speed comparison of \texttt{discrete\_2d\_convolution}
            vs \texttt{scipy.signal.convolve2d}}

\subsubsection*{Experimental setup}

We measure the average wall-clock time (over 5 runs) of two implementations:
\begin{itemize}
  \item \textbf{Custom} (\texttt{discrete\_2d\_convolution}): a pure-NumPy
        implementation using explicit Python \texttt{for}-loops over every
        output pixel, each iteration computing an element-wise product of an
        image patch with the kernel via \texttt{np.sum}.
  \item \textbf{SciPy} (\texttt{scipy.signal.convolve2d}): called with
        \texttt{mode='same'} to match the custom function's output size.
\end{itemize}

We sweep four image sizes ($64$, $128$, $256$, $512$ pixels square) and three
kernel sizes ($3\!\times\!3$, $7\!\times\!7$, $15\!\times\!15$) on random
float64 arrays.

\subsubsection*{Results}

\begin{table}[H]
  \centering
  \caption{Mean execution time (seconds) and speedup of SciPy over the custom
           implementation.}
  \label{tab:conv_benchmark}
  \begin{tabular}{ccrrrr}
    \toprule
    Image size & Kernel & Custom (s) & SciPy (s) & Speedup \\
    \midrule
    $64\times64$   & $3\times3$   & 0.0230 & 0.0001 & $\approx201\times$ \\
    $64\times64$   & $7\times7$   & 0.0215 & 0.0004 & $\approx58\times$  \\
    $64\times64$   & $15\times15$ & 0.0219 & 0.0013 & $\approx17\times$  \\
    \midrule
    $128\times128$ & $3\times3$   & 0.0840 & 0.0004 & $\approx219\times$ \\
    $128\times128$ & $7\times7$   & 0.0843 & 0.0015 & $\approx56\times$  \\
    $128\times128$ & $15\times15$ & 0.0878 & 0.0053 & $\approx17\times$  \\
    \midrule
    $256\times256$ & $3\times3$   & 0.3294 & 0.0018 & $\approx186\times$ \\
    $256\times256$ & $7\times7$   & 0.3361 & 0.0062 & $\approx54\times$  \\
    $256\times256$ & $15\times15$ & 0.3560 & 0.0216 & $\approx17\times$  \\
    \midrule
    $512\times512$ & $3\times3$   & 1.3480 & 0.0068 & $\approx198\times$ \\
    $512\times512$ & $7\times7$   & 1.3710 & 0.0247 & $\approx56\times$  \\
    $512\times512$ & $15\times15$ & 1.4760 & 0.0918 & $\approx16\times$  \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{convolution_benchmark.png}
  \caption{Execution time (seconds) as a function of image size for three
           kernel sizes. SciPy is consistently faster by one to two orders
           of magnitude.}
  \label{fig:conv_benchmark}
\end{figure}

\subsubsection*{Explanation of the speed difference}

\paragraph{Custom implementation.}
The custom function uses two nested Python \texttt{for}-loops — one over each
output pixel — for a total of $H \times W$ Python iterations.  Inside each
iteration, \texttt{np.sum(patch * kernel)} performs $K_h \times K_w$
floating-point operations in C, but the loop itself is interpreted by the
Python runtime.  The overall time complexity is
$\mathcal{O}(H \cdot W \cdot K_h \cdot K_w)$, and the constant factor is
large because Python loop overhead dominates for small kernels (the inner
NumPy call is very short-lived relative to the Python dispatch cost).

\paragraph{SciPy implementation.}
\texttt{scipy.signal.convolve2d} is implemented in compiled C/Fortran and
avoids Python-level loops entirely.  For large kernels it can additionally
switch to an FFT-based algorithm with complexity
$\mathcal{O}(H \cdot W \cdot \log(H \cdot W))$, which is asymptotically
cheaper than the direct-sum approach.

\paragraph{Why the speedup \emph{decreases} with larger kernels.}
For a small kernel (e.g.\ $3\!\times\!3$), the inner NumPy work per Python
iteration is tiny, so \emph{Python overhead} is the bottleneck and the
speedup is largest (${\approx}200\times$).  As the kernel grows (e.g.\
$15\!\times\!15$), each Python iteration triggers more NumPy work, slightly
amortising the overhead, and SciPy's own cost grows too — leading to a
smaller but still substantial speedup (${\approx}16\times$).

\paragraph{Conclusion.}
The gap illustrates why production deep-learning frameworks never implement
convolutions with Python loops.  Using compiled back-ends (and optionally
FFT-based algorithms) is essential for achieving practical training and
inference speeds.

% ─────────────────────────────────────────────────────────────────────────────
\subsection{Question 2: Blurring kernel}

\subsubsection*{Proposed kernel}

We use a \textbf{Gaussian kernel} of size $15\times15$ with standard deviation
$\sigma = 3$ pixels, defined as:

\begin{equation}
  K[m,n] = \frac{1}{Z}\exp\!\left(-\frac{m^2+n^2}{2\sigma^2}\right),
  \qquad Z = \sum_{m,n}\exp\!\left(-\frac{m^2+n^2}{2\sigma^2}\right),
  \label{eq:gaussian_kernel}
\end{equation}

where the normalisation constant $Z$ ensures the kernel sums to~$1$, so that
the overall image brightness is preserved.  The kernel is visualised in
Figure~\ref{fig:gaussian_kernel} and the blurring result in
Figure~\ref{fig:blur_result}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.45\linewidth]{gaussian_kernel.png}
  \caption{Gaussian kernel ($15\times15$, $\sigma=3$). Weights are highest at
           the centre and decay smoothly towards the edges.}
  \label{fig:gaussian_kernel}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{blur_result.png}
  \caption{Left: original image. Right: image after applying the
           $15\times15$ Gaussian blur kernel ($\sigma=3$).}
  \label{fig:blur_result}
\end{figure}

\subsubsection*{Why Gaussian convolution performs blurring}

Convolving an image with a kernel replaces every pixel value with a
\emph{weighted average} of its neighbourhood.  When the kernel weights are
non-negative and sum to~$1$ — as in Equation~\eqref{eq:gaussian_kernel} —
each output pixel becomes a mixture of nearby input pixels, which has two
immediate consequences:

\begin{enumerate}
  \item \textbf{High-frequency detail is attenuated.}  Sharp transitions
        (edges, fine textures) correspond to rapid spatial changes in pixel
        intensity — i.e.\ high-frequency components in the Fourier domain.
        Averaging over a neighbourhood dampens these abrupt changes, reducing
        high-frequency energy and producing a smoother image.

  \item \textbf{Low-frequency structure is preserved.}  Regions of slowly
        varying intensity (large uniform areas, coarse shapes) are nearly
        constant across the averaging window, so their contribution to the
        output is barely changed.
\end{enumerate}

The Gaussian weighting is preferable to a flat \emph{box} (average) kernel
because it gives more weight to pixels close to the centre and progressively
less weight to distant ones.  This smooth roll-off avoids the ringing
artefacts (Gibbs phenomenon) that a sharp rectangular window introduces, and
produces a more visually natural blur.  Formally, the Gaussian kernel is the
unique kernel that is separable, isotropic, and whose repeated application
remains Gaussian — a property inherited from the Central Limit Theorem, which
states that repeated averaging converges to a Gaussian distribution.

% ─────────────────────────────────────────────────────────────────────────────
\subsection{Question 3: Edge-detection kernels}

\subsubsection*{Proposed kernels}

We use the \textbf{Sobel operator}, a standard first-order derivative filter
that combines a finite-difference gradient with Gaussian smoothing along the
orthogonal axis.  Two $3\times3$ kernels are defined:

\begin{equation}
  G_y = \begin{bmatrix} -1 & -2 & -1 \\ 0 & 0 & 0 \\ 1 & 2 & 1 \end{bmatrix}
  \qquad
  G_x = \begin{bmatrix} -1 & 0 & 1 \\ -2 & 0 & 2 \\ -1 & 0 & 1 \end{bmatrix}
  \label{eq:sobel}
\end{equation}

$G_y$ (horizontal edges) approximates the partial derivative of the image
intensity along the vertical axis ($\partial I / \partial y$).
$G_x$ (vertical edges) approximates the partial derivative along the
horizontal axis ($\partial I / \partial x$).
Both kernels are visualised in Figure~\ref{fig:sobel_kernels}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\linewidth]{sobel_kernels.png}
  \caption{Sobel kernels $G_y$ (left) and $G_x$ (right).
           Red cells carry positive weights; blue cells carry negative weights.
           The central column/row of zeros makes the kernel anti-symmetric.}
  \label{fig:sobel_kernels}
\end{figure}

\subsubsection*{Results}

Figure~\ref{fig:edge_result} shows the original image alongside the absolute
gradient responses of the two kernels.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{edge_detection_result.png}
  \caption{Left: original image. Centre: horizontal edges ($|G_y * I|$).
           Right: vertical edges ($|G_x * I|$).
           Bright pixels indicate strong gradient responses.}
  \label{fig:edge_result}
\end{figure}

As expected, $G_y$ highlights the roof line, hood, and bumpers (predominantly
horizontal boundaries), while $G_x$ highlights the windshield pillars, wheel
arches, and side panels (predominantly vertical boundaries).

\subsubsection*{How the kernels detect edges}

Each Sobel kernel can be decomposed as the outer product of a
\emph{smoothing} vector and a \emph{differencing} vector:

\begin{equation}
  G_y = \begin{bmatrix}1\\2\\1\end{bmatrix}
        \begin{bmatrix}-1&0&1\end{bmatrix}^{\!\top}
  \hspace{2em}
  G_x = \begin{bmatrix}-1\\0\\1\end{bmatrix}
        \begin{bmatrix}1&2&1\end{bmatrix}^{\!\top}
\end{equation}

The differencing vector computes a numerical first derivative: it subtracts
pixel values on one side of a boundary from those on the other side.  Where
intensity is locally constant (flat region) the difference is zero; where
intensity changes abruptly (edge) the difference is large.  The smoothing
vector averages over the perpendicular direction, which reduces sensitivity to
noise while still localising the edge accurately.  Taking the absolute value
of the output captures both rising and falling transitions as bright ridges
in the response map.

% ─────────────────────────────────────────────────────────────────────────────
\subsection{Question 6: MLP vs MobileNet on PathMNIST}

\subsubsection*{Experimental setup}

Both models are trained on \textbf{PathMNIST} (9-class colorectal tissue
classification, $3\times28\times28$ images) using the fixed hyperparameters
in \texttt{main\_classification.py}:

\begin{itemize}
  \item Optimiser: AdamW, lr $= 10^{-3}$, weight decay $= 5\times10^{-4}$
  \item Batch size: 128, Epochs: 15, Loss: CrossEntropyLoss
  \item Data augmentation (train): random horizontal flip + random resized crop
\end{itemize}

\textbf{MLP}: four hidden layers $[1024, 512, 64, 64]$, ReLU, Glorot normal
init, flattened input of size $2352$.

\textbf{MobileNet}: depthwise separable convolutions (Table~1 of the
assignment), AdaptiveAvgPool $\to$ $1\times1$, linear head $1024\to9$.

\subsubsection*{Results}

\begin{table}[H]
  \centering
  \caption{Final test performance after 15 epochs.}
  \label{tab:q6_results}
  \begin{tabular}{lrr}
    \toprule
    Model & Best val.\ accuracy & Test accuracy \\
    \midrule
    MLP       & -- & -- \\
    MobileNet & -- & -- \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{training_curves.png}
  \caption{Training and validation loss (left) and accuracy (right) over
           15 epochs.  MLP: blue; MobileNet: orange.
           Solid lines: train, dashed lines: validation.}
  \label{fig:training_curves}
\end{figure}

\subsubsection*{Conclusion}

% TODO: fill in after training runs complete.

%══════════════════════════════════════════════════════════════════════════════
\section{Problem 3}
%══════════════════════════════════════════════════════════════════════════════

\subsection{Question 1: UNet implementation}

% TODO

\subsection{Question 2: DiceLoss and DiceCELoss}

% TODO

%══════════════════════════════════════════════════════════════════════════════
\section{Problem 4}
%══════════════════════════════════════════════════════════════════════════════

\subsection{Question 1: UNet without skip connections}

\subsubsection*{Architecture}

We build a \textbf{plain encoder-decoder} (autoencoder) with the same channel
widths as the standard UNet, but with skip connections removed.  The only
architectural difference is in the decoder blocks:

\begin{itemize}
  \item \textbf{UNet decoder block}: $\text{upconv}(C_\text{in}
        \!\to\! C_\text{out})$ $\to$ concat skip $(C_\text{out})$
        $\to$ double-conv$(2C_\text{out} \!\to\! C_\text{out})$
  \item \textbf{No-skip decoder block}: $\text{upconv}(C_\text{in}
        \!\to\! C_\text{out})$ $\to$ double-conv$(C_\text{out}
        \!\to\! C_\text{out})$
\end{itemize}

All other hyperparameters (optimizer, lr, epochs, loss) are held constant
so that the comparison isolates the effect of skip connections.

\subsubsection*{Results}

\begin{table}[H]
  \centering
  \caption{Best validation Dice score for UNet vs.\ UNet without skip
           connections (40 epochs, retina vessel segmentation).}
  \label{tab:p4q1_results}
  \begin{tabular}{lrr}
    \toprule
    Model & Best val.\ Dice \\
    \midrule
    UNet (with skip)    & -- \\
    UNet (no skip)      & -- \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{p4_q1_curves.png}
  \caption{Training and validation Dice loss (left) and Dice score (right)
           for UNet with and without skip connections.}
  \label{fig:p4q1_curves}
\end{figure}

\subsubsection*{Conclusion}

% TODO: fill in after training. Suggested talking points:
%  - UNet (with skip) should achieve a significantly higher Dice score.
%  - Without skip connections, high-frequency spatial detail (vessel edges,
%    thin structures) is lost in the bottleneck and cannot be recovered.
%  - Skip connections allow the decoder to directly reuse fine-grained
%    encoder features at each resolution level, enabling accurate localisation.
%  - The no-skip model may still segment large vessels but struggles with
%    thin capillaries and precise boundaries.

\subsection{Question 2: Effect of learning rate}

\subsubsection*{Experimental setup}

We train the UNet with the \textbf{Adam} optimizer for 40 epochs using five
learning rates: $10^{-1}$, $10^{-2}$, $10^{-3}$, $10^{-4}$, $10^{-5}$.
All other hyperparameters (batch size, weight decay, loss) are held constant.

\subsubsection*{Results}

\begin{table}[H]
  \centering
  \caption{Best validation Dice score per learning rate.}
  \label{tab:p4q2_lr}
  \begin{tabular}{lr}
    \toprule
    Learning rate & Best val.\ Dice \\
    \midrule
    $10^{-1}$ & -- \\
    $10^{-2}$ & -- \\
    $10^{-3}$ & -- \\
    $10^{-4}$ & -- \\
    $10^{-5}$ & -- \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{p4_q2_lr_curves.png}
  \caption{Validation DiceCE loss (left) and Dice score (right) for each
           learning rate across 40 epochs.}
  \label{fig:p4q2_lr}
\end{figure}

\subsubsection*{Findings}

% TODO: fill in after training. Expected observations below:
%
% lr = 0.1:  Likely diverges or oscillates heavily — steps too large,
%            loss explodes or gets stuck at a poor minimum.
%
% lr = 0.01: May converge but unstably — loss curve noisy, possible
%            overshooting around good minima.
%
% lr = 0.001: Typically the best trade-off for Adam on segmentation tasks —
%             fast convergence, stable curve, high Dice score.
%
% lr = 0.0001: Converges smoothly but slowly — reaches a good solution
%              by epoch 40 but may not fully converge.
%
% lr = 0.00001: Too slow — the model barely trains in 40 epochs, giving
%               low Dice scores close to a random or trivial prediction.
%
% General conclusion: Adam is sensitive to learning rate.  The optimal
% value balances step size (fast convergence) against stability (avoiding
% overshooting).  For this task lr ~ 1e-3 is expected to work best.

\subsection{Question 3: Data augmentation strategies}

\subsubsection*{Strategies investigated}

We compare four augmentation strategies against a no-augmentation baseline,
all applied exclusively to the training set.  Every strategy transforms the
image and mask \emph{consistently} so that spatial labels remain valid.

\begin{enumerate}
  \item \textbf{Geometric}: random horizontal flip ($p=0.5$), random vertical
        flip ($p=0.5$), random rotation $\pm30°$ with reflect padding.
        Rationale: retinal vessels have no preferred orientation; the model
        should be invariant to flips and moderate rotations.

  \item \textbf{Photometric}: random brightness offset $\in[-40,40]$ and
        random contrast scaling factor $\in[0.7,1.3]$ applied to the image
        only (masks are unchanged).
        Rationale: fundus camera calibration and illumination vary between
        patients; contrast/brightness shifts simulate this variability.

  \item \textbf{Elastic deformation}: smooth random displacement field
        ($\alpha=50$, $\sigma=6$) applied to both image and mask via
        bilinear / nearest-neighbour interpolation respectively.
        Rationale: elastic deformations simulate natural tissue deformation
        and increase effective dataset size for thin, curved structures.

  \item \textbf{Combined}: all three strategies applied in sequence
        (geometric $\to$ photometric $\to$ elastic).
        Rationale: if the three strategies address orthogonal sources of
        variation, combining them should yield the best generalisation.
\end{enumerate}

\begin{figure}[H]
  \centering
  % TODO: add sample augmented images here (one row per strategy)
  \caption{Example augmented images (top to bottom: geometric, photometric,
           elastic, combined) with corresponding masks.}
  \label{fig:p4q3_aug_examples}
\end{figure}

\subsubsection*{Results}

\begin{table}[H]
  \centering
  \caption{Best validation Dice score per augmentation strategy (40 epochs).}
  \label{tab:p4q3_aug}
  \begin{tabular}{lr}
    \toprule
    Strategy & Best val.\ Dice \\
    \midrule
    No augmentation & -- \\
    Geometric       & -- \\
    Photometric     & -- \\
    Elastic         & -- \\
    Combined        & -- \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{p4_q3_aug_curves.png}
  \caption{Validation DiceCE loss (left) and Dice score (right) for each
           augmentation strategy across 40 epochs.}
  \label{fig:p4q3_aug_curves}
\end{figure}

\subsubsection*{Conclusion}

% TODO: fill in after training. Suggested talking points:
%  - Geometric augmentation typically helps most — orientation invariance
%    is a strong prior for vessel structure.
%  - Photometric augmentation helps with domain shift between patients/cameras.
%  - Elastic deformation is particularly valuable for thin structures
%    (capillaries) where the model must learn deformable boundaries.
%  - Combined should outperform any single strategy if they address
%    independent sources of variation; if not, the added noise may hurt.
%  - The dataset is small (DRIVE: 20 train images), so augmentation has
%    a larger relative impact than on large datasets.

\subsection{Question 4: Pretrained model fine-tuning}

\subsubsection*{Setup}

We compare two models trained under identical conditions (same optimizer,
lr, epochs, loss, data):

\begin{itemize}
  \item \textbf{UNet from scratch}: our custom implementation with randomly
        initialised weights (Problem 3).
  \item \textbf{ResNet18-UNet (fine-tuned)}: \texttt{smp.Unet} from the
        \texttt{segmentation-models-pytorch} library with a ResNet-18 encoder
        pre-trained on ImageNet.  The encoder starts from ImageNet weights;
        the decoder is randomly initialised and trained from scratch.
\end{itemize}

The key difference is the encoder initialisation.  In the pretrained model,
the encoder has already learned powerful hierarchical feature representations
(edges, textures, object parts) from 1.2M ImageNet images.  Fine-tuning
adapts these representations to the retinal domain while the decoder learns
to map them to vessel segmentation masks.

\subsubsection*{Results}

\begin{table}[H]
  \centering
  \caption{Best validation Dice score: from scratch vs.\ fine-tuned.}
  \label{tab:p4q4}
  \begin{tabular}{lrr}
    \toprule
    Model & Parameters & Best val.\ Dice \\
    \midrule
    UNet (from scratch)         & -- & -- \\
    ResNet18-UNet (fine-tuned)  & -- & -- \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{p4_q4_curves.png}
  \caption{Training and validation DiceCE loss (left) and Dice score (right).
           Blue: UNet from scratch.  Orange: ResNet18-UNet fine-tuned from
           ImageNet weights.}
  \label{fig:p4q4_curves}
\end{figure}

\subsubsection*{Conclusion}

% TODO: fill in after training. Suggested talking points:
%
% Convergence speed:
%   The pretrained model should converge faster — its encoder already
%   extracts meaningful low-level features (edges, gradients) that are
%   relevant for vessel detection, requiring fewer gradient steps to
%   reach a good solution.
%
% Final performance:
%   The pretrained model should achieve a higher Dice score, especially
%   given the small training set (DRIVE: 20 images).  With so little
%   data, learning good feature representations from scratch is difficult;
%   pretrained features act as strong regularisation.
%
% Training stability:
%   The pretrained model's loss curve should be smoother — it starts
%   from a better initialisation rather than random noise.
%
% Domain gap:
%   ImageNet contains natural images, not fundus photographs.  Despite
%   this gap, low-level features (edge detectors, texture filters) transfer
%   well.  Higher-level semantic features transfer less, which is why
%   the decoder must still be trained from scratch.
%
% General principle:
%   Transfer learning is most beneficial when: (1) the source dataset is
%   large and diverse, (2) the target dataset is small, and (3) there is
%   some overlap in low-level feature structure between domains.

\end{document}
