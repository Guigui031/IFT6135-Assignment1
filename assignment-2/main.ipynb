{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTv0D26B9W2h"
      },
      "source": [
        "# Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdN5pnC8MMx-"
      },
      "source": [
        "This notebook is intended to produce the plots and figures for the report on Problem 1 of the practical. You should not run this notebook in Google Colab until you have finished constructing the correct solutions for transformer_solution.py and gru_solution.py\n",
        "\n",
        "This notebook provides some limited commentary on several HuggingFace Features and toolage. You will use HuggingFace Datasets to load the Yelp Polarity dataset for sentiment analysis. The notebook will define a Bert tokenizer, collate functions, and then train and evaluate several models using the HuggingFace utilities mentioned above. Remember, the most crucial part here is running the experiments for the report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mount your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFHMMDtSwuW4",
        "outputId": "8db247e6-4231-4915-d44a-1c4e30c72766"
      },
      "outputs": [],
      "source": [
        "# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
        "# you can delete this cell which is specific to Google Colab. You may also\n",
        "# change the paths for data/logs in Arguments below.\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!pip install -qqq datasets transformers textattack --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Link your assignment folder & install requirements\n",
        "Enter the path to the assignment folder in your Google Drive\n",
        "If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
        "you can delete this cell which is specific to Google Colab. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oODLwt1QzgGa"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import warnings\n",
        "\n",
        "folder = \"\" #@param {type:\"string\"}\n",
        "!ln -Ts \"$folder\" /content/assignment 2> /dev/null\n",
        "!cp gdrive/MyDrive/Assignment2/transformer_solution.py .\n",
        "!cp gdrive/MyDrive/Assignment2/gru_solution.py .\n",
        "\n",
        "# Add the assignment folder to Python path\n",
        "if '/content/assignment' not in sys.path:\n",
        "  sys.path.insert(0, '/content/assignment')\n",
        "\n",
        "# Check if CUDA is available\n",
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "  warnings.warn('CUDA is not available.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dt3NTvpsy4Oc"
      },
      "source": [
        "### Running on GPU\n",
        "For this assignment, it will be necessary to run your experiments on GPU. To make sure the notebook is running on GPU, you can change the notebook settings with\n",
        "* (EN) `Edit > Notebook Settings`\n",
        "* (FR) `Modifier > ParamÃ¨tres du notebook`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "RLVSmv9HoMH5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "from typing import List, Dict, Union, Optional, Tuple\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Optimizer, AdamW\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from datasets import Dataset, load_dataset, concatenate_datasets\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "from transformer_solution import Transformer, MultiHeadedAttention\n",
        "from gru_solution import EncoderDecoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed: int = 0, device: torch.device = None):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if device is not None and device.type == \"cuda\":\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    return rng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"--> Device selected: {device}\")\n",
        "rng = set_seed(seed=0, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_name = \"yelp_polarity\"\n",
        "dataset_train_original = load_dataset(dataset_name, split=\"train\", cache_dir=\"assignment/data\")\n",
        "dataset_test_original = load_dataset(dataset_name, split=\"test\", cache_dir=\"assignment/data\").shuffle(generator=rng)\n",
        "dataset_test = dataset_test_original.select(range(1000))\n",
        "dataset_train = concatenate_datasets([\n",
        "    dataset_train_original,\n",
        "    dataset_test_original.select(range(1000, len(dataset_test_original)))\n",
        "])\n",
        "print(f\"{len(dataset_train)=}, {len(dataset_test)=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ” Quick look at the data\n",
        "Lets have quick look at a few samples in our test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_samples_to_see = 3\n",
        "for i in range(n_samples_to_see):\n",
        "  print(\"-\"*30)\n",
        "  print(\"title:\", dataset_test[i][\"text\"])\n",
        "  print(\"label:\", dataset_test[i][\"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzxwRDQFUtaG"
      },
      "source": [
        "### 1ï¸. Tokenize the `text`\n",
        "Tokenize the `text`portion of each sample (i.e. parsing the text to smaller chunks). Tokenization can happen in many ways; traditionally, this was done based on the white spaces. With transformer-based models, tokenization is performed based on the frequency of occurrence of \"chunk of text\". This frequency can be learned in many different ways. However the most common one is the [**wordpiece**](https://arxiv.org/pdf/1609.08144v2.pdf) model. \n",
        "> The wordpiece model is generated using a data-driven approach to maximize the language-model likelihood\n",
        "of the training data, given an evolving word definition. Given a training corpus and a number of desired\n",
        "tokens $D$, the optimization problem is to select $D$ wordpieces such that the resulting corpus is minimal in the\n",
        "number of wordpieces when segmented according to the chosen wordpiece model.\n",
        "\n",
        "Under this model:\n",
        "1. Not all things can be converted to tokens depending on the model. For example, most models have been pretrained without any knowledge of emojis. So their token will be `[UNK]`, which stands for unknown.\n",
        "2. Some words will be mapped to multiple tokens!\n",
        "3. Depending on the kind of model, your tokens may or may not respect capitalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qCpNwaTYSo3U"
      },
      "outputs": [],
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OMDqabyToBt",
        "outputId": "f3477698-69e0-41e6-c840-beca5796e5c3"
      },
      "outputs": [],
      "source": [
        "input_sample = \"Welcome to IFT6135. We now teach you ðŸ¤—(HUGGING FACE) Library :DDD.\"\n",
        "tokenizer.tokenize(input_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEu6aqReXqp6"
      },
      "source": [
        "### 2. Encoding\n",
        "Once we have tokenized the text, we then need to convert these chuncks to numbers so we can feed them to our model. This conversion is basically a look-up in a dictionary **from `str` $\\to$ `int`**. The tokenizer object can also perform this work. While it does so it will also add the *special* tokens needed by the model to the encodings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpDGccrvYKnT",
        "outputId": "f5776da3-28fa-4fc2-d247-d231214395e8"
      },
      "outputs": [],
      "source": [
        "input_sample = \"Welcome to IFT6135. We now teach you ðŸ¤—(HUGGING FACE) Library :DDD.\" #@param {type: \"string\"}\n",
        "\n",
        "print(\"--> Token Encodings:\\n\",tokenizer.encode(input_sample))\n",
        "print(\"-.\"*15)\n",
        "print(\"--> Token Encodings Decoded:\\n\",tokenizer.decode(tokenizer.encode(input_sample)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI8lFKZSZ2ZW"
      },
      "source": [
        "### 3ï¸. Truncate/Pad samples\n",
        "Since all the sample in the batch will not have the same sequence length, we would need to truncate the longer sequences (i.e. the ones that exeed a predefined maximum length) and pad the shorter ones so we that we can equal length for all the samples in the batch. Once this is achieved, we would need to convert the result to `torch.Tensor`s and return. These tensors will then be retrieved from the [dataloader](https://https//pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Collate:\n",
        "    def __init__(self, model_name: str, max_len: int) -> None:\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.max_len = max_len\n",
        "        self.text_column = \"text\"\n",
        "        self.label_column = \"label\"\n",
        "\n",
        "    def __call__(self, batch: List[Dict[str, Union[str, int]]]) -> Dict[str, torch.Tensor]:\n",
        "        texts = list(map(lambda batch_instance: batch_instance[self.text_column], batch))\n",
        "        tokenized_inputs = self.tokenizer(\n",
        "            texts,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\",\n",
        "            return_token_type_ids=False,\n",
        "        )\n",
        "\n",
        "        labels = list(map(lambda batch_instance: int(batch_instance[self.label_column]), batch))\n",
        "        labels = torch.LongTensor(labels)\n",
        "        return dict(tokenized_inputs, **{\"labels\": labels})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ðŸ§‘â€ðŸ³ Setting up the collate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VaSpuyIjNqn"
      },
      "outputs": [],
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "sample_max_length = 256         #@param {type: \"integer\"}\n",
        "collate = Collate(model_name=model_name, max_len=sample_max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "y9P4oWyOSexA"
      },
      "outputs": [],
      "source": [
        "class ReviewClassifierPretrained(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone: str = \"bert-base-uncased\",\n",
        "        backbone_hidden_size: int = 768,\n",
        "        num_classes: int = 2,\n",
        "        device: torch.device = torch.device(\"cpu\"),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.backbone_hidden_size = backbone_hidden_size\n",
        "        self.num_classes = num_classes\n",
        "        self.device = device\n",
        "        self.back_bone = AutoModel.from_pretrained(\n",
        "            self.backbone,\n",
        "            output_attentions=False,\n",
        "            output_hidden_states=False,\n",
        "        )\n",
        "        for parameter in self.back_bone.parameters():\n",
        "            parameter.requires_grad= False\n",
        "        self.classifier = torch.nn.Linear(self.backbone_hidden_size, self.num_classes)\n",
        "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        labels: Optional[torch.Tensor] = None\n",
        "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        with torch.no_grad():\n",
        "            self.back_bone.eval()\n",
        "            back_bone_output = self.back_bone(input_ids.to(self.device), attention_mask=attention_mask.to(self.device))\n",
        "        hidden_states = back_bone_output[0]\n",
        "        pooled_output = hidden_states[:, 0]  # getting the [CLS] token\n",
        "        logits = self.classifier(pooled_output)\n",
        "        loss = self.loss_fn(\n",
        "            logits.view(-1, self.num_classes),\n",
        "            labels.view(-1).to(self.device),\n",
        "        )\n",
        "        return loss, logits\n",
        "\n",
        "\n",
        "class ReviewClassifierRNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 2,\n",
        "        vocabulary_size: int = 30522,\n",
        "        encoder_only: bool = False,\n",
        "        dropout: float = 0.5,\n",
        "        embed_dim: int = 256,\n",
        "        with_attn: bool = True,\n",
        "        device: torch.device = torch.device(\"cpu\"),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.encoder_only = encoder_only\n",
        "        self.device = device\n",
        "        self.back_bone = EncoderDecoder(\n",
        "            vocabulary_size=vocabulary_size,\n",
        "            dropout=dropout,\n",
        "            encoder_only=encoder_only,\n",
        "            with_attn=with_attn,\n",
        "        )\n",
        "        self.classifier = torch.nn.Linear(embed_dim, self.num_classes)\n",
        "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        labels: Optional[torch.Tensor] = None\n",
        "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        pooled_output, _ = self.back_bone(input_ids.to(self.device), attention_mask.to(self.device))\n",
        "        logits = self.classifier(pooled_output)\n",
        "        loss = self.loss_fn(\n",
        "            logits.view(-1, self.num_classes),\n",
        "            labels.view(-1).to(self.device),\n",
        "        )\n",
        "        return loss, logits\n",
        "\n",
        "\n",
        "class ReviewClassifierTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 2,\n",
        "        vocabulary_size: int = 30522,\n",
        "        sequence_length: int = 256,\n",
        "        num_heads: int = 4,\n",
        "        num_layers: int = 4,\n",
        "        block: str=\"prenorm\",\n",
        "        embed_dim: int = 256,\n",
        "        hidden_dim: int = 256,\n",
        "        dropout: float = 0.3,\n",
        "        device: torch.device = torch.device(\"cpu\"),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.device = device\n",
        "        self.back_bone = Transformer(\n",
        "            vocabulary_size=vocabulary_size,\n",
        "            sequence_length=sequence_length,\n",
        "            embed_dim=embed_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_layers,\n",
        "            block=block,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.classifier = torch.nn.Linear(embed_dim, self.num_classes)\n",
        "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        labels: Optional[torch.Tensor] = None\n",
        "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        pooled_output = self.back_bone(input_ids.to(self.device), attention_mask.to(self.device))\n",
        "        logits = self.classifier(pooled_output)\n",
        "        loss = self.loss_fn(\n",
        "            logits.view(-1, self.num_classes),\n",
        "            labels.view(-1).to(self.device),\n",
        "        )\n",
        "        return loss, logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(\n",
        "    model: torch.nn.Module,\n",
        "    train_dataloader: DataLoader,\n",
        "    test_dataloader: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    logging_frequency: int,\n",
        "    logs: dict,\n",
        "    device: torch.device,\n",
        "    epoch_idx: int = 0,\n",
        "):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    logging_loss = 0\n",
        "    epoch_start_time = time.time()\n",
        "    logfreq_start_time = time.time()\n",
        "    num_train_batch = len(train_dataloader)\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        loss, _ = model(**batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "        # print(f\"{step=}, {current_lr=}\")\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        logging_loss += loss.item()\n",
        "\n",
        "        if (step + 1) % logging_frequency == 0 or (step + 1) == num_train_batch:\n",
        "            logfreq_time = time.time() - logfreq_start_time\n",
        "            logs[\"train_time_accum\"].append(logfreq_time + logs[\"train_time_accum\"][-1])\n",
        "            logs[\"train_loss_logfreq\"].append(logging_loss / logging_frequency)\n",
        "            (\n",
        "                eval_acc,\n",
        "                tp_rate,\n",
        "                fp_rate,\n",
        "                tn_rate,\n",
        "                fn_rate,\n",
        "                f1,\n",
        "                eval_loss,\n",
        "                eval_time,\n",
        "            ) = evaluate(model=model, test_dataloader=test_dataloader, device=device)\n",
        "            logs[\"eval_acc_logfreq\"].append(eval_acc)\n",
        "            logs[\"eval_tp_rate_logfreq\"].append(tp_rate)\n",
        "            logs[\"eval_fp_rate_logfreq\"].append(fp_rate)\n",
        "            logs[\"eval_tn_rate_logfreq\"].append(tn_rate)\n",
        "            logs[\"eval_fn_rate_logfreq\"].append(fn_rate)\n",
        "            logs[\"eval_f1_score_logfreq\"].append(f1)\n",
        "            logs[\"eval_loss_logfreq\"].append(eval_loss)\n",
        "            logs[\"eval_time_accum\"].append(eval_time + logs[\"eval_time_accum\"][-1])\n",
        "            print(\n",
        "                f\"Epoch {epoch_idx+1} step {step+1}/{num_train_batch}: \"\n",
        "                f\"train time {logs[\"train_time_accum\"][-1]:.1f} seconds, \"\n",
        "                f\"train loss {logs[\"train_loss_logfreq\"][-1]:.3f}, \"\n",
        "                f\"eval time {eval_time:.1f}, eval loss {eval_loss:.3f}, eval acc {eval_acc:.3f}, \"\n",
        "                f\"eval tp rate {tp_rate:.3f}, eval fp rate {fp_rate:.3f}, eval tn rate {tn_rate:.3f}, eval fn rate {fn_rate:.3f}, eval f1 score {f1:.3f}, \"\n",
        "                f\"lr {current_lr:.3e}\")\n",
        "\n",
        "            logging_loss = 0\n",
        "            logfreq_start_time = time.time()\n",
        "\n",
        "            if (step + 1) == num_train_batch:\n",
        "                logs[\"eval_loss_epoch\"].append(eval_loss)\n",
        "                logs[\"eval_time_epoch\"].append(eval_time)\n",
        "                logs[\"eval_acc_epoch\"].append(eval_acc)\n",
        "\n",
        "    train_loss_epoch = epoch_loss / num_train_batch\n",
        "    train_time_epoch = time.time() - epoch_start_time\n",
        "    logs[\"train_loss_epoch\"].append(train_loss_epoch)\n",
        "    logs[\"train_time_epoch\"].append(train_time_epoch)\n",
        "    print(f\"Epoch {epoch_idx+1}: train time epoch: {train_time_epoch:.1f}, train loss epoch mean: {train_loss_epoch:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    model: torch.nn.Module,\n",
        "    test_dataloader: DataLoader,\n",
        "    device: torch.device,\n",
        "):\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(test_dataloader):\n",
        "            loss, logits = model(**batch)\n",
        "            eval_loss += loss.item()\n",
        "            predictions = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
        "            y_pred.extend(predictions)\n",
        "            y_true.extend(batch[\"labels\"].cpu().numpy())\n",
        "    y_pred, y_true = np.array(y_pred, dtype=np.float32), np.array(y_true, dtype=np.float32)\n",
        "    # print(f\"{sum(y_true==1)=}, {sum(y_true==0)=}\")\n",
        "    accuracy = np.sum(y_pred == y_true) / len(y_true)\n",
        "    tp_rate = np.sum((y_true == 1) & (y_pred == 1)) / np.sum(y_true == 1)\n",
        "    fp_rate = np.sum((y_true == 0) & (y_pred == 1)) / np.sum(y_true == 0)\n",
        "    tn_rate = np.sum((y_true == 0) & (y_pred == 0)) / np.sum(y_true == 0)\n",
        "    fn_rate = np.sum((y_true == 1) & (y_pred == 0)) / np.sum(y_true == 1)\n",
        "    f1 = f1_score(y_true=y_true, y_pred=y_pred)\n",
        "    eval_loss = eval_loss / len(test_dataloader)\n",
        "    model.train()\n",
        "    return (\n",
        "        accuracy,\n",
        "        tp_rate,\n",
        "        fp_rate,\n",
        "        tn_rate,\n",
        "        fn_rate,\n",
        "        f1,\n",
        "        eval_loss,\n",
        "        time.time() - start_time\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reset_logs(model):\n",
        "    logs = dict()\n",
        "    logs[\"parameters\"] = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
        "    print(f\"Number of parameters: {logs['parameters']}\")\n",
        "\n",
        "    logs[\"train_time_accum\"] = [0]\n",
        "    logs[\"train_loss_logfreq\"] = []\n",
        "    logs[\"train_loss_epoch\"] = []\n",
        "    logs[\"train_time_epoch\"] = []\n",
        "\n",
        "    logs[\"eval_time_accum\"] = [0]\n",
        "    logs[\"eval_acc_logfreq\"] = []\n",
        "    logs[\"eval_tp_rate_logfreq\"] = []\n",
        "    logs[\"eval_fp_rate_logfreq\"] = []\n",
        "    logs[\"eval_tn_rate_logfreq\"] = []\n",
        "    logs[\"eval_fn_rate_logfreq\"] = []\n",
        "    logs[\"eval_f1_score_logfreq\"] = []\n",
        "    logs[\"eval_loss_logfreq\"] = []\n",
        "    logs[\"eval_loss_epoch\"] = []\n",
        "    logs[\"eval_time_epoch\"] = []\n",
        "    logs[\"eval_acc_epoch\"] = []\n",
        "    return logs\n",
        "\n",
        "\n",
        "def save_logs(dictionary, log_dir, exp_id):\n",
        "    log_dir = os.path.join(log_dir, exp_id)\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    # Log arguments\n",
        "    with open(os.path.join(log_dir, \"args.json\"), \"w\") as f:\n",
        "      json.dump(dictionary, f, indent=2)\n",
        "\n",
        "def save_model(model, log_dir, exp_id):\n",
        "  log_dir = os.path.join(log_dir, exp_id)\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  # Save model\n",
        "  torch.save(model.state_dict(), f\"assignment/models/model_{exp_id}.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Problem 2\n",
        "Feel free to modify this code however it is convenient for you to produce a report except for the model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 512            #@param {type: \"integer\"}\n",
        "logging_frequency = 25      #@param {type: \"integer\"}\n",
        "learning_rate = 1e-4        #@param {type: \"number\"}\n",
        "\n",
        "sample_max_length = 256     #@param {type: \"integer\"}\n",
        "experimental_setting = 1    #@param {type: \"integer\"}\n",
        "num_epochs = 5              #@param {type: \"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if experimental_setting == 1:\n",
        "    print(\"GRU: no dropout, encoder only\")\n",
        "    model = ReviewClassifierRNN(\n",
        "        dropout=0.0,\n",
        "        encoder_only=True,\n",
        "        device=device,\n",
        "    )\n",
        "elif experimental_setting == 2:\n",
        "    print(\"GRU: dropout, encoder only\")\n",
        "    model = ReviewClassifierRNN(\n",
        "        dropout=0.3,\n",
        "        encoder_only=True,\n",
        "        device=device,\n",
        "    )\n",
        "elif experimental_setting == 3:\n",
        "    print(\"GRU: dropout, encoder-decoder, no attn\")\n",
        "    model = ReviewClassifierRNN(\n",
        "        dropout=0.3,\n",
        "        encoder_only=False,\n",
        "        with_attn=False,\n",
        "        device=device,\n",
        "    )\n",
        "elif experimental_setting == 4:\n",
        "    print(\"GRU: dropout, encoder-decoder, with attn\")\n",
        "    model = ReviewClassifierRNN(\n",
        "        dropout=0.3,\n",
        "        encoder_only=False,\n",
        "        with_attn=True,\n",
        "        device=device,\n",
        "    )\n",
        "elif experimental_setting == 5:\n",
        "    print(\"Transformer: 2 layers, prenorm\")\n",
        "    model = ReviewClassifierTransformer(\n",
        "        sequence_length=sample_max_length,\n",
        "        num_heads=4,\n",
        "        num_layers=2,\n",
        "        block=\"prenorm\",\n",
        "        dropout=0.3,\n",
        "        device=device,\n",
        "    )\n",
        "elif experimental_setting == 6:\n",
        "    print(\"Transformer: 4 layers, prenorm\")\n",
        "    model = ReviewClassifierTransformer(\n",
        "        sequence_length=sample_max_length,\n",
        "        num_heads=4,\n",
        "        num_layers=4,\n",
        "        block=\"prenorm\",\n",
        "        dropout=0.3,\n",
        "        device=device,\n",
        "    )\n",
        "elif experimental_setting == 7:\n",
        "    print(\"Transformer: 2 layers, postnorm\")\n",
        "    model = ReviewClassifierTransformer(\n",
        "        sequence_length=sample_max_length,\n",
        "        num_heads=4,\n",
        "        num_layers=2,\n",
        "        block=\"postnorm\",\n",
        "        dropout=0.3,\n",
        "        device=device,\n",
        "    )\n",
        "elif experimental_setting == 8:\n",
        "    print(\"Pretrained BERT\")\n",
        "    model = ReviewClassifierPretrained(\n",
        "        backbone=model_name,\n",
        "        device=device,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setting up the optimizer\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logs = reset_logs(model)\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(\n",
        "        model=model,\n",
        "        train_dataloader=train_dataloader,\n",
        "        test_dataloader=test_dataloader,\n",
        "        optimizer=optimizer,\n",
        "        logging_frequency=logging_frequency,\n",
        "        logs=logs,\n",
        "        device=device,\n",
        "        epoch_idx=epoch,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_logs(logs, \"assignment/log\", str(experimental_setting))\n",
        "save_model(model, \"assignment/models\", str(experimental_setting))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
